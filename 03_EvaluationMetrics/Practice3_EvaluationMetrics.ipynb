{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **Evaluation Metrics** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "How do you know if your model is truly performing well? Evaluation metrics provide the answer. They act as a scorecard, measuring your model’s ability to make accurate and meaningful predictions. From Accuracy to F-score, these metrics highlight strengths, reveal weaknesses, and guide improvements.  \n",
    "\n",
    "## 1. Confusion Matrix, TP, FP, TN, and FN  \n",
    "\n",
    "The **confusion matrix** is a table that summarizes how successful the classification model is at predicting examples belonging to various classes.\n",
    "The key terms are:  \n",
    "\n",
    "- **True Positives (TP):** Correctly predicted positive cases.  \n",
    "- **False Positives (FP):** Incorrectly predicted as positive (false alarms).  \n",
    "- **True Negatives (TN):** Correctly predicted negative cases.  \n",
    "- **False Negatives (FN):** Incorrectly predicted as negative (missed positives).  \n",
    "<center>\n",
    "\n",
    "|                 | **Predicted: 0** | **Predicted: 1** |  \n",
    "|-----------------|--------------------|--------------------|  \n",
    "| **Actual: 0** | True Negative (TN)   | False Positive (FP)  |  \n",
    "| **Actual: 1** | False Negative (FN)  | True Positive (TP)   |  \n",
    "\n",
    "</center>\n",
    "\n",
    "## 2. Evaluation Metrics  \n",
    "\n",
    "### Accuracy:  \n",
    "The proportion of correct predictions out of the total predictions.  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}\n",
    "$$  \n",
    "\n",
    "### Precision:  \n",
    "How many of the predicted positive cases are truly positive.  \n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$  \n",
    "\n",
    "### Recall:  \n",
    "How many of the actual positive cases are correctly predicted.  \n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$  \n",
    "\n",
    "### F-score:  \n",
    "The harmonic mean of Precision and Recall, balancing the two.  \n",
    "$$\n",
    "\\text{F-score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FP} + \\text{FN}}\n",
    "$$  \n",
    "\n",
    "## 3. Illustrative Example:\n",
    "Imagine during the COVID-19 epidemy, you develop a testing kit that can determine if a person has COVID or not.\n",
    "\n",
    "- TP: People tested positive and actually have covid\n",
    "\n",
    "- TN: People tested negative and really do not have covid\n",
    "\n",
    "- FP: People tested positive but do not have covid\n",
    "\n",
    "- FN: People tested negative but actually have covid (Extremely dangerous)\n",
    "\n",
    "- Accuracy: Of all the people tested, how many are correctly diagnosed (TP + TF / n)\n",
    "\n",
    "- **Precision**: Of all positively-tested people, how many truly have covid ? (TP / TP + FP)\n",
    "\n",
    "- **Recall**: Of all the people having covid, how many were spotted ? (TP / TP + FN).\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/metrics.jpeg\" width=\"57%\">\n",
    "</center>\n",
    "\n",
    "# Exercise  \n",
    "\n",
    "Given the following confusion matrix:\n",
    "\n",
    "<center>\n",
    "\n",
    "|     n = 192     | **Predicted: 0** | **Predicted: 1** |  \n",
    "|-----------------|--------------------|--------------------|  \n",
    "| **Actual: 0** | 118                 | 12                 |  \n",
    "| **Actual: 1** | 47                  | 15                 |  \n",
    "\n",
    "</center>\n",
    "\n",
    "Compute the Accuracy, Precision, Recall, and F-score for this model.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6927\n",
      "Precision: 0.5556\n",
      "Recall: 0.2419\n",
      "F_score: 0.3371\n"
     ]
    }
   ],
   "source": [
    "TP = 15\n",
    "FP = 12\n",
    "TN = 118\n",
    "FN = 47\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F_score = 2 * (Precision*Recall) / (Precision+Recall)\n",
    "\n",
    "print(f\"Accuracy: {Accuracy:.4f}\")\n",
    "print(f\"Precision: {Precision:.4f}\")\n",
    "print(f\"Recall: {Recall:.4f}\")\n",
    "print(f\"F_score: {F_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<p>\n",
    "    This document was created in Jupyter Notebook by <span style=\"color:red;\">Trần Minh Dương (tmd)</span>.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    If you have any questions or notice any errors, feel free to reach out via Discord at \n",
    "    <span style=\"color:blue;\">@tmdhoctiengphap</span> or <span style=\"color:red;\">@ICT-Supporters</span> on the USTH Learning Support server.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Check out my GitHub repository for more projects: \n",
    "    <a href=\"https://github.com/GalaxyAnnihilator/MachineLearning\" target=\"_blank\">\n",
    "        GalaxyAnnihilator/MachineLearning\n",
    "    </a>.\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
