{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **Neural Networks** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "Neural networks are powerful models inspired by the human brain, capable of learning complex patterns from data. They consist of interconnected layers of neurons that process information and transform inputs into meaningful outputs. From image recognition to natural language processing, this document aims to uncover how neural networks are trained.\n",
    "\n",
    "## 1. Architecture  \n",
    "\n",
    "A typical neural network is composed of the following:  \n",
    "\n",
    "- **Input Layer**: Receives raw data (features) and feeds it into the network.  \n",
    "- **Hidden Layers**: Performs computations with weights, biases, and activation functions to uncover patterns in the data.  \n",
    "- **Output Layer**: Produces the final result, such as a class label or prediction value.  \n",
    "\n",
    "<center>  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\" alt=\"Neural Network Diagram\" width=\"40%\" />\n",
    "</center>  \n",
    "\n",
    "## 2. Learning Process  \n",
    "\n",
    "The network trains through these steps:  \n",
    "\n",
    "1. **Forward Pass**: Let data flow through the network, producing predictions.  \n",
    "2. **Loss Calculation**: A loss function measures prediction errors.  \n",
    "3. **Back Propagation**: Errors are propagated backward to adjust weights and biases.  \n",
    "4. **Optimization**: Algorithms like Gradient Descent minimize loss, improving predictions.\n",
    "\n",
    "## 3. Forward Pass\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/forwardpass.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "To calculate the next node in a neural network, we do the following:\n",
    "\n",
    "- Calculate the weighted sum of the previous nodes $x_i$:\n",
    "\n",
    "$$ x_j = \\sum_{i} x_{i} \\cdot w_{ij} = x_{1} \\cdot w_{1j} + x_{2} \\cdot w_{2j} + ...$$\n",
    "\n",
    "- Pass it through the activation function:\n",
    "\n",
    "$$ y_j = f(x_j) $$\n",
    "\n",
    "To further understand, see exercise below\n",
    "\n",
    "## 4.Backpropagation (Mad Difficult)\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/backpropagation.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "#### Step 1. Error Calculation\n",
    "The loss function measures the difference between the predicted output and the actual output:  \n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{output}} - y_{\\text{actual}})^2\n",
    "$$\n",
    "\n",
    "*(Note that this loss function varies for different problems: A regression problem may use MSE while a classification one may use log-likelihood.)*\n",
    "\n",
    "#### Step 2. Error Signal for the Output Layer\n",
    "The error signal at the output layer neuron is:  \n",
    "$$\n",
    "\\delta_{\\text{output}} =  \\frac{\\partial \\text{Loss}}{\\partial x_{\\text{output}}} = \\frac{\\partial \\text{Loss}}{\\partial y_{\\text{output}}} \\cdot \\frac{\\partial y_{\\text{output}}}{\\partial x_{\\text{output}}} = (y_{\\text{output}} - y_{\\text{actual}}) \\cdot F'(x_{\\text{output}})\n",
    "$$\n",
    "\n",
    "#### Step 3. Error Signal for Hidden Layer Neurons\n",
    "The error signal for a hidden layer neuron $j$ is backpropagated from the next layer:\n",
    "$$\n",
    "\\delta_j = \\frac{\\partial L}{\\partial x_j} = \\sum_{k \\in \\text{next layer}} \\frac{\\partial L}{\\partial x_k} \\cdot \\frac{\\partial x_k}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial x_j} =  \\sum_{k \\in \\text{next layer}} \\delta_k \\cdot w_{jk} \\cdot F'(x_j)\n",
    "$$\n",
    "Where:  \n",
    "- $\\delta_k$ is the error signal from the next layer.  \n",
    "- $w_{jk}$ is the weight connecting neuron $j$ to neuron $k$.  \n",
    "- $F'(x_j)$ is the derivative of the activation function of the hidden neuron.\n",
    "\n",
    "#### Step 4. Weight Update Rule\n",
    "The weights are updated to minimize the loss using the gradient descent formula:  \n",
    "$$\n",
    "w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} - \\eta \\cdot \\delta_j \\cdot y_i\n",
    "$$\n",
    "Where:  \n",
    "- $\\eta$ is the learning rate.  \n",
    "- $\\delta_j$ is the error signal of neuron $j$.  \n",
    "- $y_i$ is the output of the previous neuron (input to the current layer).  \n",
    "\n",
    "#### Step 5. Bias Update Rule (if applicable)\n",
    "Biases are updated similarly to weights:  \n",
    "$$\n",
    "b_j^{\\text{new}} = b_j^{\\text{old}} - \\eta \\cdot \\delta_j\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Activation Function Derivative**\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/activation-functions-8.png\" width = \"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise**  \n",
    "Given the following neural network:  \n",
    "\n",
    "<center>  \n",
    "<img src = \"../images/neuralnetworkexercise.png\" width = \"60%\">  \n",
    "</center>  \n",
    "\n",
    "**Weights:**  \n",
    "- $ w_{13} = 2 $, $ w_{23} = -3 $  \n",
    "- $ w_{14} = 1 $, $ w_{24} = 4 $  \n",
    "- $ w_{35} = 2 $, $ w_{45} = -1 $  \n",
    "\n",
    "**Activation Function (ReLU):**  \n",
    "$$  \n",
    "\\text{ReLU}(v) = \n",
    "\\begin{cases} \n",
    "v & \\text{if } v \\geq 0 \\\\\n",
    "0 & \\text{if } v < 0  \n",
    "\\end{cases}  \n",
    "$$  \n",
    "**Derivative of ReLU:**  \n",
    "$$  \n",
    "\\text{ReLU}'(v) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } v > 0 \\\\\n",
    "0 & \\text{if } v \\leq 0  \n",
    "\\end{cases}  \n",
    "$$  \n",
    "\n",
    "**Questions:**  \n",
    "a) Predict the output with inputs: $ x_1 = 1 $ and $ x_2 = -1 $.  \n",
    "b) Using back-propagation, evaluate the weights after one iteration with $ \\eta = 0.1 $.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Solution:**  \n",
    "\n",
    "### a) Predict the output with inputs: $ x_1 = 1 $ and $ x_2 = -1 $  \n",
    "\n",
    "**Forward Pass:**  \n",
    "1. **At node $ y_3 $:**  \n",
    "   $$  \n",
    "   x_3 = x_1 \\cdot w_{13} + x_2 \\cdot w_{23} = 1 \\cdot 2 + (-1) \\cdot (-3) = 5  \n",
    "   $$  \n",
    "   $$  \n",
    "   y_3 = \\text{ReLU}(x_3) = 5 \\quad (\\text{since } x_3 > 0)  \n",
    "   $$  \n",
    "\n",
    "2. **At node $ y_4 $:**  \n",
    "   $$  \n",
    "   x_4 = x_1 \\cdot w_{14} + x_2 \\cdot w_{24} = 1 \\cdot 1 + (-1) \\cdot 4 = -3  \n",
    "   $$  \n",
    "   $$  \n",
    "   y_4 = \\text{ReLU}(x_4) = 0 \\quad (\\text{since } x_4 < 0)  \n",
    "   $$  \n",
    "\n",
    "3. **At node $ y_5 $:**  \n",
    "   $$  \n",
    "   x_5 = y_3 \\cdot w_{35} + y_4 \\cdot w_{45} = 5 \\cdot 2 + 0 \\cdot (-1) = 10  \n",
    "   $$  \n",
    "   $$  \n",
    "   y_{\\text{output}} = \\text{ReLU}(x_5) = 10 \\quad (\\text{since } x_5 > 0)  \n",
    "   $$  \n",
    "\n",
    "**Prediction:**  \n",
    "$$  \n",
    "y_{\\text{output}} = 10  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### b) Back-propagation: Weight Updates  \n",
    "\n",
    "**Loss Function:**  \n",
    "$$  \n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{output}} - y_{\\text{actual}})^2 = \\frac{1}{2} \\cdot (10 - 0)^2 = 50  \n",
    "$$  \n",
    "\n",
    "**Gradient Formula for Any Weight $ w_{ij} $:**  \n",
    "$$  \n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{ij}} = \\frac{\\partial \\text{Loss}}{\\partial y_{\\text{output}}} \\cdot \\frac{\\partial y_{\\text{output}}}{\\partial x_{\\text{output}}} \\cdot \\frac{\\partial x_{\\text{output}}}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial x_j} \\cdot \\frac{\\partial x_j}{\\partial w_{ij}}  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Update Output Layer Weights ($ w_{35}, w_{45} $):**  \n",
    "\n",
    "**For $ w_{35} $:**  \n",
    "$$\\boxed{ \n",
    "y_3 \\underset{w_{35}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$ \n",
    "\n",
    "By the **chain rule**:  \n",
    "\n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{35}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial w_{35}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot y_3 \\\\\n",
    "&= 10 \\cdot 1 \\cdot 5 \\\\\n",
    "&= 50\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "**Update:**  \n",
    "$$  \n",
    "w_{35}^{\\text{new}} = w_{35} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{35}} = 2 - 0.1 \\cdot 50 = -3  \n",
    "$$  \n",
    "\n",
    "**For $ w_{45} $:**  \n",
    "$$\\boxed{ \n",
    "y_4 \\underset{w_{45}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$ \n",
    "\n",
    "By the **chain rule**:  \n",
    "\n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{45}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial w_{45}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot y_4 \\\\\n",
    "&= 10 \\cdot 1 \\cdot 0 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "**Update:**  \n",
    "$$  \n",
    "w_{45}^{\\text{new}} = w_{45} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{45}} = -1 - 0.1 \\cdot 0 = -1  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Update Hidden Layer Weights ($ w_{13}, w_{23} $):**  \n",
    "\n",
    "**For $ w_{13} $:** \n",
    "$$\\boxed{ \n",
    "x_1 \\underset{w_{13}}{\\rightarrow} x_3 \\underset{\\text{ReLU}}{\\rightarrow} y_3 \\underset{w_{35}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$  \n",
    "\n",
    "By the **chain rule**:  \n",
    "\n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{13}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_3} \\cdot \\frac{\\partial y_3}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial w_{13}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot w_{35} \\cdot \\text{ReLU}'(x_3) \\cdot x_1 \\\\\n",
    "&= 10 \\cdot 1 \\cdot 2 \\cdot 1 \\cdot 1 \\\\\n",
    "&= 20\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "**Update:**  \n",
    "$$  \n",
    "w_{13}^{\\text{new}} = w_{13} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{13}} = 2 - 0.1 \\cdot 20 = 0  \n",
    "$$  \n",
    "\n",
    "**For $ w_{23} $:**\n",
    "$$\\boxed{ \n",
    "x_2 \\underset{w_{23}}{\\rightarrow} x_3 \\underset{\\text{ReLU}}{\\rightarrow} y_3 \\underset{w_{35}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$  \n",
    "\n",
    "By the **chain rule**:  \n",
    "\n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{23}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_3} \\cdot \\frac{\\partial y_3}{\\partial x_3} \\cdot \\frac{\\partial x_3}{\\partial w_{23}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot w_{35} \\cdot \\text{ReLU}'(x_3) \\cdot x_2 \\\\\n",
    "&= 10 \\cdot 1 \\cdot 2 \\cdot 1 \\cdot -1 \\\\\n",
    "&= -20\n",
    "\\end{align*}\n",
    "$$    \n",
    " \n",
    "**Update:**  \n",
    "$$  \n",
    "w_{23}^{\\text{new}} = w_{23} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{23}} = -3 - 0.1 \\cdot (-20) = -1  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Update Hidden Layer Weights ($ w_{14}, w_{24} $):**  \n",
    "\n",
    "**For $ w_{14} $:**  \n",
    "$$\\boxed{ \n",
    "x_1 \\underset{w_{14}}{\\rightarrow} x_4 \\underset{\\text{ReLU}}{\\rightarrow} y_4 \\underset{w_{45}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$  \n",
    "\n",
    "By the **chain rule**:   \n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{14}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_4} \\cdot \\frac{\\partial y_4}{\\partial x_4} \\cdot \\frac{\\partial x_4}{\\partial w_{14}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot w_{45} \\cdot \\text{ReLU}'(x_4) \\cdot x_1 \\\\\n",
    "&= 10 \\cdot 1 \\cdot (-1) \\cdot 0 \\cdot 1 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "**Update:**  \n",
    "$$  \n",
    "w_{14}^{\\text{new}} = w_{14} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{14}} = 1 - 0.1 \\cdot 0 = 1  \n",
    "$$  \n",
    "\n",
    "\n",
    "**For $ w_{24} $:**  \n",
    "$$\\boxed{ \n",
    "x_2 \\underset{w_{24}}{\\rightarrow} x_4 \\underset{\\text{ReLU}}{\\rightarrow} y_4 \\underset{w_{45}}{\\rightarrow} x_5 \\underset{\\text{ReLU}}{\\rightarrow} y_5 \\underset{\\text{MSE}}{\\rightarrow} \\text{Loss}}\n",
    "$$  \n",
    "\n",
    "By the **chain rule**:  \n",
    "$$  \n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_{24}} &= \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_4} \\cdot \\frac{\\partial y_4}{\\partial x_4} \\cdot \\frac{\\partial x_4}{\\partial w_{24}} \\\\\n",
    "&= (y_5 - y_{\\text{actual}}) \\cdot \\text{ReLU}'(x_5) \\cdot w_{45} \\cdot \\text{ReLU}'(x_4) \\cdot x_2 \\\\\n",
    "&= 10 \\cdot 1 \\cdot (-1) \\cdot 0 \\cdot (-1) \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "**Update:**  \n",
    "$$  \n",
    "w_{24}^{\\text{new}} = w_{24} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{24}} = 4 - 0.1 \\cdot 0 = 4  \n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Updated Weights**  \n",
    "- $ w_{13} = \\boxed{0} $, $ w_{23} = \\boxed{-1} $  \n",
    "- $ w_{14} = \\boxed{1} $, $ w_{24} = \\boxed{4} $  \n",
    "- $ w_{35} = \\boxed{-3} $, $ w_{45} = \\boxed{-1} $  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights:\n",
      "w13: -0.0\n",
      "w23: -1.0\n",
      "w14: 1.0\n",
      "w24: 4.0\n",
      "w35: -3.0\n",
      "w45: -1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #weights\n",
    "        self.w13 = torch.tensor(2.0, requires_grad=True)\n",
    "        self.w23 = torch.tensor(-3.0, requires_grad=True)\n",
    "        self.w14 = torch.tensor(1.0, requires_grad=True)\n",
    "        self.w24 = torch.tensor(4.0, requires_grad=True)\n",
    "        self.w35 = torch.tensor(2.0, requires_grad=True)\n",
    "        self.w45 = torch.tensor(-1.0, requires_grad=True)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Forward pass\n",
    "        x3 = x1 * self.w13 + x2 * self.w23\n",
    "        y3 = torch.relu(x3)\n",
    "        \n",
    "        x4 = x1 * self.w14 + x2 * self.w24\n",
    "        y4 = torch.relu(x4)\n",
    "        \n",
    "        x5 = y3 * self.w35 + y4 * self.w45\n",
    "        y5 = torch.relu(x5)\n",
    "        \n",
    "        return y5\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "# Inputs and target\n",
    "x1 = torch.tensor(1.0)\n",
    "x2 = torch.tensor(-1.0)\n",
    "y_actual = torch.tensor(0.0)  # Actual value\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.1\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD([model.w13, model.w23, model.w14, model.w24, model.w35, model.w45], lr=eta)\n",
    "\n",
    "# Forward pass\n",
    "y5 = model(x1, x2)\n",
    "\n",
    "# Compute the loss\n",
    "loss = 0.5 * (y5 - y_actual) ** 2\n",
    "\n",
    "# Back Propagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Updated Weights:\")\n",
    "print(f\"w13: {model.w13.item():.1f}\")\n",
    "print(f\"w23: {model.w23.item()}\")\n",
    "print(f\"w14: {model.w14.item()}\")\n",
    "print(f\"w24: {model.w24.item()}\")\n",
    "print(f\"w35: {model.w35.item()}\")\n",
    "print(f\"w45: {model.w45.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<p>\n",
    "    This document was created in Jupyter Notebook by <span style=\"color:red;\">Trần Minh Dương (tmd)</span>.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    If you have any questions or notice any errors, feel free to reach out via Discord at \n",
    "    <span style=\"color:blue;\">@tmdhoctiengphap</span> or <span style=\"color:red;\">@ICT-Supporters</span> on the USTH Learning Support server.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Check out my GitHub repository for more projects: \n",
    "    <a href=\"https://github.com/GalaxyAnnihilator/MachineLearning\" target=\"_blank\">\n",
    "        GalaxyAnnihilator/MachineLearning\n",
    "    </a>.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
