{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **Neural Networks** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "Neural networks are powerful models inspired by the human brain, capable of learning complex patterns from data. They consist of interconnected layers of neurons that process information and transform inputs into meaningful outputs. From image recognition to natural language processing, neural networks are a cornerstone of modern Artificial Intelligence (AI).  \n",
    "\n",
    "## 1. Architecture  \n",
    "\n",
    "A typical neural network is composed of the following:  \n",
    "\n",
    "- **Input Layer**: Receives raw data (features) and feeds it into the network.  \n",
    "- **Hidden Layers**: Performs computations with weights, biases, and activation functions to uncover patterns in the data.  \n",
    "- **Output Layer**: Produces the final result, such as a class label or prediction value.  \n",
    "\n",
    "<center>  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\" alt=\"Neural Network Diagram\" width=\"40%\" />\n",
    "</center>  \n",
    "\n",
    "## 2. Learning Process  \n",
    "\n",
    "The network trains through these steps:  \n",
    "\n",
    "1. **Forward Pass**: Let data flow through the network, producing predictions.  \n",
    "2. **Loss Calculation**: A loss function measures prediction errors.  \n",
    "3. **Back Propagation**: Errors are propagated backward to adjust weights and biases.  \n",
    "4. **Optimization**: Algorithms like Gradient Descent minimize loss, improving predictions.\n",
    "\n",
    "## 3. Forward Pass\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/forwardpass.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "To calculate the next node in a neural network, we do the following:\n",
    "\n",
    "- Calculate the weighted sum of $x_i$ with $w_i$:\n",
    "\n",
    "$$ x_j = \\sum_{i} x_{i} \\cdot w_{ij} = x_{1} \\cdot w_{1j} + x_{2} \\cdot w_{2j} + ...$$\n",
    "\n",
    "- Pass it through the activation function:\n",
    "\n",
    "$$ y_j = f(x_j) $$\n",
    "\n",
    "To further understand, see exercise below\n",
    "\n",
    "## 4.Backpropagation (Mad Difficult)\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/backpropagation.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "#### Step 1. Error Calculation\n",
    "The loss function measures the difference between the predicted output and the actual output:  \n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{predicted}} - y_{\\text{actual}})^2\n",
    "$$\n",
    "\n",
    "*(Note that this loss function varies for different problems: A regression problem may use MSE while a classification one may use log-likelihood.)*\n",
    "\n",
    "#### Step 2. Error Signal for the Output Layer\n",
    "The error signal at the output layer neuron is:  \n",
    "$$\n",
    "\\delta_{\\text{output}} =  \\frac{\\partial \\text{Loss}}{\\partial v_{\\text{output}}} = \\frac{\\partial \\text{Loss}}{\\partial y_{\\text{predicted}}} \\cdot \\frac{\\partial y_{\\text{predicted}}}{\\partial v_{\\text{output}}} = (y_{\\text{predicted}} - y_{\\text{actual}}) \\cdot F'(v_{\\text{output}})\n",
    "$$\n",
    "Where:  \n",
    "- $v_{\\text{output}}$ is the weighted sum of inputs to the output neuron.  \n",
    "- $F'(v_{\\text{output}})$ is the derivative of the activation function.\n",
    "\n",
    "#### Step 3. Error Signal for Hidden Layer Neurons\n",
    "The error signal for a hidden layer neuron $j$ is backpropagated from the next layer:\n",
    "$$\n",
    "\\delta_j = \\frac{\\partial L}{\\partial v_j} = \\sum_{k \\in \\text{next layer}} \\frac{\\partial L}{\\partial v_k} \\cdot \\frac{\\partial v_k}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial v_j} =  \\sum_{k \\in \\text{next layer}} \\delta_k \\cdot w_{jk} \\cdot F'(v_j)\n",
    "$$\n",
    "Where:  \n",
    "- $\\delta_k$ is the error signal from the next layer.  \n",
    "- $w_{jk}$ is the weight connecting neuron $j$ to neuron $k$.  \n",
    "- $F'(v_j)$ is the derivative of the activation function of the hidden neuron.\n",
    "\n",
    "#### Step 4. Weight Update Rule\n",
    "The weights are updated to minimize the loss using the gradient descent formula:  \n",
    "$$\n",
    "w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} - \\eta \\cdot \\delta_j \\cdot y_i\n",
    "$$\n",
    "Where:  \n",
    "- $\\eta$ is the learning rate.  \n",
    "- $\\delta_j$ is the error signal of neuron $j$.  \n",
    "- $y_i$ is the output of the previous neuron (input to the current layer).  \n",
    "\n",
    "#### Step 5. Bias Update Rule (if applicable)\n",
    "Biases are updated similarly to weights:  \n",
    "$$\n",
    "b_j^{\\text{new}} = b_j^{\\text{old}} - \\eta \\cdot \\delta_j\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Activation Function Derivative**\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/activation-functions-8.png\" width = \"70%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Given the following neural network:\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/neuralnetworkexercise.png\" width = \"70%\">\n",
    "</center>\n",
    "\n",
    "Weights:\n",
    "\n",
    "$w_{13} = 2 , w_{23} = -3 $\n",
    "\n",
    "$w_{14} = 1 , w_{24} = 4 $\n",
    "\n",
    "$w_{35} = 2 , w_{45} = -1 $\n",
    "\n",
    "The activation function is ReLU:\n",
    "$$\n",
    "F(v) = \n",
    "\\begin{cases} \n",
    "v & \\text{if } v \\geq 0 \\\\\n",
    "0 & \\text{if } v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Derivative of ReLU\n",
    "$$\n",
    "F'(v) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } v > 0 \\\\\n",
    "0 & \\text{if } v \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Questions:\n",
    "\n",
    "a) Predict the output with inputs: $x_1 = 1$ and $x_2 = 2$\n",
    "\n",
    "b) Knowing that the actual output is y = 0, the coss function is given as:\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{predicted}} - y_{\\text{actual}})^2\n",
    "$$\n",
    "\n",
    "Using back-propagation, evaluate the weights after one iteration with $\\eta = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<p>\n",
    "    This document was created in Jupyter Notebook by <span style=\"color:red;\">Trần Minh Dương (tmd)</span>.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    If you have any questions or notice any errors, feel free to reach out via Discord at \n",
    "    <span style=\"color:blue;\">@tmdhoctiengphap</span> or <span style=\"color:red;\">@ICT-Supporters</span> on the USTH Learning Support server.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Check out my GitHub repository for more projects: \n",
    "    <a href=\"https://github.com/GalaxyAnnihilator/MachineLearning\" target=\"_blank\">\n",
    "        GalaxyAnnihilator/MachineLearning\n",
    "    </a>.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
