{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **Neural Networks** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "Neural networks are powerful models inspired by the human brain, capable of learning complex patterns from data. They consist of interconnected layers of neurons that process information and transform inputs into meaningful outputs. From image recognition to natural language processing, this document aims to uncover how neural networks are trained.\n",
    "\n",
    "## 1. Architecture  \n",
    "\n",
    "A typical neural network is composed of the following:  \n",
    "\n",
    "- **Input Layer**: Receives raw data (features) and feeds it into the network.  \n",
    "- **Hidden Layers**: Performs computations with weights, biases, and activation functions to uncover patterns in the data.  \n",
    "- **Output Layer**: Produces the final result, such as a class label or prediction value.  \n",
    "\n",
    "<center>  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\" alt=\"Neural Network Diagram\" width=\"40%\" />\n",
    "</center>  \n",
    "\n",
    "## 2. Learning Process  \n",
    "\n",
    "The network trains through these steps:  \n",
    "\n",
    "1. **Forward Pass**: Let data flow through the network, producing predictions.  \n",
    "2. **Loss Calculation**: A loss function measures prediction errors.  \n",
    "3. **Back Propagation**: Errors are propagated backward to adjust weights and biases.  \n",
    "4. **Optimization**: Algorithms like Gradient Descent minimize loss, improving predictions.\n",
    "\n",
    "## 3. Forward Pass\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/forwardpass.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "To calculate the next node in a neural network, we do the following:\n",
    "\n",
    "- Calculate the weighted sum of the previous nodes $x_i$:\n",
    "\n",
    "$$ x_j = \\sum_{i} x_{i} \\cdot w_{ij} = x_{1} \\cdot w_{1j} + x_{2} \\cdot w_{2j} + ...$$\n",
    "\n",
    "- Pass it through the activation function:\n",
    "\n",
    "$$ y_j = f(x_j) $$\n",
    "\n",
    "To further understand, see exercise below\n",
    "\n",
    "## 4.Backpropagation (Mad Difficult)\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/backpropagation.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "#### Step 1. Error Calculation\n",
    "The loss function measures the difference between the predicted output and the actual output:  \n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{output}} - y_{\\text{actual}})^2\n",
    "$$\n",
    "\n",
    "*(Note that this loss function varies for different problems: A regression problem may use MSE while a classification one may use log-likelihood.)*\n",
    "\n",
    "#### Step 2. Error Signal for the Output Layer\n",
    "The error signal at the output layer neuron is:  \n",
    "$$\n",
    "\\delta_{\\text{output}} =  \\frac{\\partial \\text{Loss}}{\\partial x_{\\text{output}}} = \\frac{\\partial \\text{Loss}}{\\partial y_{\\text{output}}} \\cdot \\frac{\\partial y_{\\text{output}}}{\\partial x_{\\text{output}}} = (y_{\\text{output}} - y_{\\text{actual}}) \\cdot F'(x_{\\text{output}})\n",
    "$$\n",
    "\n",
    "#### Step 3. Error Signal for Hidden Layer Neurons\n",
    "The error signal for a hidden layer neuron $j$ is backpropagated from the next layer:\n",
    "$$\n",
    "\\delta_j = \\frac{\\partial L}{\\partial x_j} = \\sum_{k \\in \\text{next layer}} \\frac{\\partial L}{\\partial x_k} \\cdot \\frac{\\partial x_k}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial x_j} =  \\sum_{k \\in \\text{next layer}} \\delta_k \\cdot w_{jk} \\cdot F'(x_j)\n",
    "$$\n",
    "Where:  \n",
    "- $\\delta_k$ is the error signal from the next layer.  \n",
    "- $w_{jk}$ is the weight connecting neuron $j$ to neuron $k$.  \n",
    "- $F'(x_j)$ is the derivative of the activation function of the hidden neuron.\n",
    "\n",
    "#### Step 4. Weight Update Rule\n",
    "The weights are updated to minimize the loss using the gradient descent formula:  \n",
    "$$\n",
    "w_{ij}^{\\text{new}} = w_{ij}^{\\text{old}} - \\eta \\cdot \\delta_j \\cdot y_i\n",
    "$$\n",
    "Where:  \n",
    "- $\\eta$ is the learning rate.  \n",
    "- $\\delta_j$ is the error signal of neuron $j$.  \n",
    "- $y_i$ is the output of the previous neuron (input to the current layer).  \n",
    "\n",
    "#### Step 5. Bias Update Rule (if applicable)\n",
    "Biases are updated similarly to weights:  \n",
    "$$\n",
    "b_j^{\\text{new}} = b_j^{\\text{old}} - \\eta \\cdot \\delta_j\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Activation Function Derivative**\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/activation-functions-8.png\" width = \"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise**\n",
    "Given the following neural network:\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/neuralnetworkexercise.png\" width = \"60%\">\n",
    "</center>\n",
    "\n",
    "Weights:\n",
    "\n",
    "$w_{13} = 2 , w_{23} = -3 $\n",
    "\n",
    "$w_{14} = 1 , w_{24} = 4 $\n",
    "\n",
    "$w_{35} = 2 , w_{45} = -1 $\n",
    "\n",
    "The activation function is ReLU:\n",
    "$$\n",
    "F(v) = \n",
    "\\begin{cases} \n",
    "v & \\text{if } v \\geq 0 \\\\\n",
    "0 & \\text{if } v < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Derivative of ReLU\n",
    "$$\n",
    "F'(v) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } v > 0 \\\\\n",
    "0 & \\text{if } v \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Questions:\n",
    "\n",
    "a) Predict the output with inputs: $x_1 = 1$ and $x_2 = -1$\n",
    "\n",
    "b) Knowing that the actual output is y = 0, the loss function is given as:\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{output}} - y_{\\text{actual}})^2\n",
    "$$\n",
    "\n",
    "Using back-propagation, evaluate the weights after one iteration with $\\eta = 0.1$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution:**\n",
    "\n",
    "### a) Predict the output with inputs: $ x_1 = 1 $ and $ x_2 = -1 $\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "1. **At the node $ y_3 $:**\n",
    "   $$\n",
    "   x_3 = x_1 \\cdot w_{13} + x_2 \\cdot w_{23} = 1 \\cdot 2 + (-1) \\cdot (-3) = 5\n",
    "   $$\n",
    "   $$\n",
    "   y_3 = F(x_3) = 5\n",
    "   $$\n",
    "\n",
    "2. **At the node $ y_4 $:**\n",
    "   $$\n",
    "   x_4 = x_1 \\cdot w_{14} + x_2 \\cdot w_{24} = 1 \\cdot 1 + (-1) \\cdot 4 = -3\n",
    "   $$\n",
    "   $$\n",
    "   y_4 = F(x_4) = 0\n",
    "   $$\n",
    "\n",
    "3. **At the node $ y_5 $:**\n",
    "   $$\n",
    "   x_5 = y_3 \\cdot w_{35} + y_4 \\cdot w_{45} = 5 \\cdot 2 + 0 \\cdot (-1) = 10\n",
    "   $$\n",
    "   $$\n",
    "   y_5 = F(x_5) = 10\n",
    "   $$\n",
    "\n",
    "**Prediction:**  \n",
    "The output of the neural network is:  \n",
    "$$\n",
    "y_{\\text{output}} = y_5 = 10\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### b) Back-propagation\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/neuralnetworkexercise2.png\" width = \"80%\">\n",
    "</center>\n",
    "\n",
    "**Given:**  \n",
    "- Actual output: $ y = 0 $  \n",
    "- Loss function:\n",
    "  $$\n",
    "  \\text{Loss} = \\frac{1}{2} \\cdot (y_{\\text{output}} - y_{\\text{actual}})^2\n",
    "  $$\n",
    "\n",
    "**Error at the output node $ y_5 $:**\n",
    "$$\n",
    "\\delta_5 =  \\frac{\\partial \\text{Loss}}{\\partial x_5} = \\frac{\\partial \\text{Loss}}{\\partial y_5} \\cdot \\frac{\\partial y_5}{\\partial x_5} = (y_5 - y_{\\text{actual}}) \\cdot F'(x_5)\n",
    "$$\n",
    "$$\n",
    "\\delta_5 = (10 - 0) \\cdot F'(10) = 10\n",
    "$$\n",
    "\n",
    "**Update weights $ w_{35} $ and $ w_{45} $:**\n",
    "$$\n",
    "w_{35} = w_{35} - \\eta \\cdot \\delta_5 \\cdot y_3 = 2 - 0.1 \\cdot 10 \\cdot 5 = -3\n",
    "$$\n",
    "$$\n",
    "w_{45} = w_{45} - \\eta \\cdot \\delta_5 \\cdot y_4 = -1 - 0.1 \\cdot 10 \\cdot 0 = -1\n",
    "$$\n",
    "\n",
    "**Error at the node $ y_3 $:**\n",
    "$$\n",
    "\\delta_3 =  \\frac{\\partial \\text{Loss}}{\\partial x_3} = \\frac{\\partial \\text{Loss}}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_3} \\cdot \\frac{\\partial y_3}{\\partial x_3} = \\delta_5 \\cdot w_{35} \\cdot F'(x_3)\n",
    "$$\n",
    "$$\n",
    "\\delta_3 = 10 \\cdot 2 \\cdot F'(5) = 20\n",
    "$$\n",
    "\n",
    "**Update weights $ w_{13} $ and $ w_{23} $:**\n",
    "$$\n",
    "w_{13} = w_{13} - \\eta \\cdot \\delta_3 \\cdot x_1 = 2 - 0.1 \\cdot 20 \\cdot 1 = 0\n",
    "$$\n",
    "$$\n",
    "w_{23} = w_{23} - \\eta \\cdot \\delta_3 \\cdot x_2 = -3 - 0.1 \\cdot 20 \\cdot (-1) = -1\n",
    "$$\n",
    "\n",
    "**Error at the node $ y_4 $:**\n",
    "$$\n",
    "\\delta_4 =  \\frac{\\partial \\text{Loss}}{\\partial x_4} = \\frac{\\partial \\text{Loss}}{\\partial x_5} \\cdot \\frac{\\partial x_5}{\\partial y_4} \\cdot \\frac{\\partial y_4}{\\partial x_4} = \\delta_5 \\cdot w_{45} \\cdot F'(x_4)\n",
    "$$\n",
    "$$\n",
    "\\delta_4 = 10 \\cdot (-1) \\cdot F'(-3) = 0\n",
    "$$\n",
    "\n",
    "**Update weights $ w_{14} $ and $ w_{24} $:**\n",
    "$$\n",
    "w_{14} = w_{14} - \\eta \\cdot \\delta_4 \\cdot x_1 = 1 - 0.1 \\cdot 0 \\cdot 1 = 1\n",
    "$$\n",
    "$$\n",
    "w_{24} = w_{24} - \\eta \\cdot \\delta_4 \\cdot x_2 = 4 - 0.1 \\cdot 0 \\cdot (-1) = 4\n",
    "$$\n",
    "\n",
    "### Final updated weights:\n",
    "- $ w_{13} = 0, \\, w_{23} = -1 $  \n",
    "- $ w_{14} = 1, \\, w_{24} = 4 $  \n",
    "- $ w_{35} = -3, \\, w_{45} = -1 $\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass outputs: {'x3': 5, 'y3': 5, 'x4': -3, 'y4': 0, 'x5': 10, 'y5': 10}\n",
      "Updated weights: {'w13': 0.0, 'w23': -1.0, 'w14': 1.0, 'w24': 4.0, 'w35': -3.0, 'w45': -1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Weights\n",
    "weights = {\n",
    "    \"w13\": 2, \"w23\": -3,\n",
    "    \"w14\": 1, \"w24\": 4,\n",
    "    \"w35\": 2, \"w45\": -1\n",
    "}\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return x>0\n",
    "\n",
    "x1, x2 = 1, -1\n",
    "y_actual = 0\n",
    "eta = 0.1\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(weights, x1, x2):\n",
    "    x3 = x1 * weights[\"w13\"] + x2 * weights[\"w23\"]\n",
    "    y3 = relu(x3)\n",
    "\n",
    "    x4 = x1 * weights[\"w14\"] + x2 * weights[\"w24\"]\n",
    "    y4 = relu(x4)\n",
    "\n",
    "    x5 = y3 * weights[\"w35\"] + y4 * weights[\"w45\"]\n",
    "    y5 = relu(x5)\n",
    "\n",
    "    return {\"x3\": x3, \"y3\": y3, \"x4\": x4, \"y4\": y4, \"x5\": x5, \"y5\": y5}\n",
    "\n",
    "def back_propagation(weights, forward_values, x1, x2, y_actual, eta):\n",
    "    y5 = forward_values[\"y5\"]\n",
    "    x3, y3 = forward_values[\"x3\"], forward_values[\"y3\"]\n",
    "    x4, y4 = forward_values[\"x4\"], forward_values[\"y4\"]\n",
    "    x5 = forward_values[\"x5\"]\n",
    "\n",
    "    # Compute error signals\n",
    "    delta5 = (y5 - y_actual) * relu_derivative(x5)\n",
    "    delta3 = delta5 * weights[\"w35\"] * relu_derivative(x3)\n",
    "    delta4 = delta5 * weights[\"w45\"] * relu_derivative(x4)\n",
    "\n",
    "    # Update weights\n",
    "    weights[\"w35\"] -= eta * delta5 * y3\n",
    "    weights[\"w45\"] -= eta * delta5 * y4\n",
    "\n",
    "    weights[\"w13\"] -= eta * delta3 * x1\n",
    "    weights[\"w23\"] -= eta * delta3 * x2\n",
    "\n",
    "    weights[\"w14\"] -= eta * delta4 * x1\n",
    "    weights[\"w24\"] -= eta * delta4 * x2\n",
    "\n",
    "    return weights\n",
    "\n",
    "forward_values = forward_pass(weights, x1, x2)\n",
    "print(\"Forward pass outputs:\", forward_values)\n",
    "\n",
    "updated_weights = back_propagation(weights, forward_values, x1, x2, y_actual, eta)\n",
    "print(\"Updated weights:\", updated_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<p>\n",
    "    This document was created in Jupyter Notebook by <span style=\"color:red;\">Trần Minh Dương (tmd)</span>.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    If you have any questions or notice any errors, feel free to reach out via Discord at \n",
    "    <span style=\"color:blue;\">@tmdhoctiengphap</span> or <span style=\"color:red;\">@ICT-Supporters</span> on the USTH Learning Support server.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Check out my GitHub repository for more projects: \n",
    "    <a href=\"https://github.com/GalaxyAnnihilator/MachineLearning\" target=\"_blank\">\n",
    "        GalaxyAnnihilator/MachineLearning\n",
    "    </a>.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
