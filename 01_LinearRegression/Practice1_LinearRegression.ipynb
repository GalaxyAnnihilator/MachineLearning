{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **Linear Regression** </center>\n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>\n",
    "\n",
    "# **Overview**  \n",
    "\n",
    "Imagine you're trying to predict house prices based on features like the size of the house, the number of bedrooms, and the location. Linear regression helps you establish a relationship between these features (inputs) and the price (output) by finding the best-fit line that minimizes the prediction error.  \n",
    "\n",
    "<center><img src = \"../images/linear_regression_diagram.png\" width=\"500\"></center>\n",
    "\n",
    "Linear regression offers a simple yet powerful way to model relationships in data. To find this best-fit line, we use one of two methods:\n",
    "1. **Gradient Descent:** An iterative optimization process that adjusts the model step by step.  \n",
    "2. **Normal Equation:** A direct solution using linear algebra to calculate the optimal parameters.  \n",
    "\n",
    "In this document, we’ll explore both methods in detail and understand their applications.\n",
    "\n",
    "## 1. Gradient Descent\n",
    "Gradient descent is an iterative optimization algorithm used to minimize the cost function in linear regression.\n",
    "\n",
    "### Key Points:\n",
    "**Hypothesis Function:** The predicted output of linear regression is computed as follows:\n",
    "\n",
    "- **For a single example:**  \n",
    "    $$ \n",
    "    h_\\theta(x^{(i)}) = \\theta^T x^{(i)} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + \\dots \n",
    "    $$  \n",
    "    Here, $ x^{(i)} $ is a single input example.\n",
    "\n",
    "- **For the entire dataset (input matrix):**  \n",
    "    $$ \n",
    "    h_\\theta(X) = X \\theta \n",
    "    $$  \n",
    "\n",
    "  Where:\n",
    "  - $ x^{(i)} $: A column vector representing the $ i $-th example, containing $ n $ features.  \n",
    "    **Dimension:** $ n \\times 1 $.  \n",
    "  - $ X $: The input matrix containing $ m $ examples and $ n $ features.  \n",
    "    **Dimension:** $ m \\times n $.  \n",
    "  - $ \\theta $: The parameter vector containing $ n $ weights (including $ \\theta_0 $, the bias term).  \n",
    "    **Dimension:** $ n \\times 1 $.\n",
    "\n",
    "**Goal:** Minimize the cost function (Mean Squared Error) to find the optimal parameters $ \\theta $.\n",
    "\n",
    "**Mean Squared Error (MSE) Formula:**\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "  Where:\n",
    "  + $ m $: Number of training examples in the batch.\n",
    "  + $ h_\\theta(x^{(i)}) $: Predicted output for the $ i $-th example.\n",
    "  + $ y^{(i)} $: Actual output for the $ i $-th example.\n",
    "\n",
    "**Steps for Batch Gradient Descent of Linear Regression:**\n",
    "1. Start with initial values for the parameters $ \\theta $ (e.g., $ \\theta_0 = 0, \\theta_1 = 0 $).\n",
    "2. Compute the gradient of the cost function $ J(\\theta) $ with respect to each parameter:\n",
    "\n",
    "  - **Scalar form**:\n",
    "\n",
    "      $$\\boxed{\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) \\cdot x^{(i)}_j}  $$\n",
    "    where:\n",
    "      + $h_\\theta(x^{(i)})$ is the predicted output\n",
    "      + $h_\\theta(x^{(i)}) - y^{(i)}$ is the error of the prediction compared to the real output\n",
    "      + $x^{(i)}_j$ is the j-th feature value of the i-th training example\n",
    "  \n",
    "  - **Matrix form:**\n",
    "\n",
    "    $$\n",
    "    \\boxed{\\nabla J(\\theta) = \\frac{1}{m} X^T \\left( X\\theta - y \\right)}\n",
    "    $$\n",
    "    where:\n",
    "      + $X$ is the input matrix of dimension m x n.\n",
    "      + $\\theta$ is the column vector of dimension n x 1.\n",
    "      + $X\\theta$ gives the vector of predicted outputs of dimension m x 1.\n",
    "      + $y$ is the vector of target values of dimension m x1.\n",
    "\n",
    "3. Update each parameter using the formula:\n",
    "\n",
    "    $$\\boxed{\n",
    "      \\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \n",
    "      \\quad \\text{or} \\quad \n",
    "      \\theta = \\theta - \\alpha \\nabla J(\\theta)\n",
    "    }\n",
    "    $$\n",
    "\n",
    "    where $ \\alpha $ is the learning rate.        \n",
    "4. Repeat until the cost function converges.\n",
    "\n",
    "**Advantages:**\n",
    "- Works well with large datasets.\n",
    "- Scales to high-dimensional data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires choosing a suitable learning rate $\\alpha$.\n",
    "- May converge slowly or get stuck in local minima if poorly initialized.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Normal Equation\n",
    "The normal equation is a closed-form solution for linear regression. It directly computes the optimal parameters $ \\theta $ without iteration.\n",
    "\n",
    "### Key Points:\n",
    "**Goal:** Minimize the cost function by solving:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T (X\\theta - y) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T X \\theta = X^T y\n",
    "$$\n",
    "\n",
    "- Finally, we derive the formula for $\\theta$:\n",
    "\n",
    "    $$\n",
    "    \\boxed{\\theta = (X^T X)^{-1} X^T y}\n",
    "    $$\n",
    "\n",
    "   where:\n",
    "    + $ X $: Feature matrix.\n",
    "    + $ y $: Vector of target values.\n",
    "    + $ X^T $: Transpose of $ X $.\n",
    "    + $ (X^T X)^{-1} $: Inverse of $ X^T X $.\n",
    "\n",
    "**Steps:**\n",
    "  1. Construct the feature matrix $ X $ (including the column of 1s for the intercept).\n",
    "  2. Compute $ \\theta $ using the formula above.\n",
    "\n",
    "**Advantages:**\n",
    "  - No need for choosing a learning rate.\n",
    "\n",
    "  - Direct computation provides exact results.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "  - Computationally expensive for large datasets due to matrix inversion ($ O(n^3) $).\n",
    "\n",
    "  - May cause error when $ X^T X $ is non-invertible or the dataset is too large to fit into memory.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Key Differences\n",
    "\n",
    "| Aspect               | Gradient Descent               | Normal Equation              |\n",
    "|----------------------|--------------------------------|-----------------------------|\n",
    "| **Approach**         | Iterative optimization         | Direct computation          |\n",
    "| **Computation Cost** | Scales well for large datasets | Expensive for large datasets |\n",
    "| **Parameters Required** | Requires learning rate ($ \\alpha $) | No hyperparameters          |\n",
    "| **Suitability**      | Large datasets or high dimensions | Small to medium datasets    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise**\n",
    "\n",
    "Given the training data for a linear regression problem, to predict the final score of a student in Machine Learning class, based on his/her score in programming and probability class. Starting at:\n",
    "\n",
    "- $\\theta_{0}$ = 0, $\\theta_{1}$ = 1, $\\theta_{2}$ = -1\n",
    "- Learning rate ($\\alpha$) = 4\n",
    "\n",
    "### Task:\n",
    "1. Calculate the coefficients after the first iteration with **batch-gradient descent**.\n",
    "2. Based on the coefficients you just found, calculate the prediction score of the fifth student.\n",
    "\n",
    "### Data Table:\n",
    "| Student | Score in Programming | Score in Probability | Score in Machine Learning |\n",
    "|---------|-----------------------|----------------------|---------------------------|\n",
    "| 1       | 18                   | 15                   | 15                        |\n",
    "| 2       | 15                   | 10                   | 11                        |\n",
    "| 3       | 16                   | 12                   | 13                        |\n",
    "| 4       | 19                   | 16                   | 17                        |\n",
    "| 5       | 17                   | 11                   | ???                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this exercise, I will use the matrix approach with the numpy library in python\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input / Output\n",
    "X = np.array([[18,15],[15,10],[16,12],[19,16]])\n",
    "y = np.array([15,11,13,17])\n",
    "\n",
    "#Parameters\n",
    "theta = np.array([0,1,-1])\n",
    "alpha = 4\n",
    "m = len(y) #batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_bias = \n",
      " [[ 1. 18. 15.]\n",
      " [ 1. 15. 10.]\n",
      " [ 1. 16. 12.]\n",
      " [ 1. 19. 16.]]\n"
     ]
    }
   ],
   "source": [
    "#reshape X to add an additional column of 1 to the left (bias term)\n",
    "X_bias = np.hstack((np.ones((m,1)),X))\n",
    "print(\"X_bias = \\n\",X_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5. 4. 3.]\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(X,theta):\n",
    "    return X @ theta # the @ here is the operator for matrix multiplication in python\n",
    "\n",
    "predictions = hypothesis(X_bias,theta)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.  -6.  -9. -14.]\n"
     ]
    }
   ],
   "source": [
    "errors = predictions - y\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -10.25 -179.   -143.  ]\n"
     ]
    }
   ],
   "source": [
    "gradient = 1/m * (X_bias.T @ errors)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta after one iteration of batch-gradient descent is: \n",
      " [ 41. 717. 571.]\n"
     ]
    }
   ],
   "source": [
    "theta = theta - alpha * gradient\n",
    "print(\"Theta after one iteration of batch-gradient descent is: \\n\",theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the fifth student with 17 in Programming and 11 in Probability is: 18511.0\n"
     ]
    }
   ],
   "source": [
    "fifthStudent = np.array([1,17,11])\n",
    "\n",
    "score = hypothesis(fifthStudent,theta)\n",
    "print(\"The score of the fifth student with 17 in Programming and 11 in Probability is:\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Normal equation approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta from the normal equation: [-9.8  1.4 -0. ]\n"
     ]
    }
   ],
   "source": [
    "X_bias = np.hstack((np.ones((m,1)),X))\n",
    "\n",
    "# Compute theta = (X^T . X)^(-1) . X^T . y\n",
    "# np.linalg.inv(A) returns the inverse of A : A^-1\n",
    "theta_normal = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y\n",
    "print(\"Theta from the normal equation:\", np.round(theta_normal,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<p>This document is written in Jupyter Notebook by <span style=\"color:red\">Trần Minh Dương(tmd) </span> - Learning Support. </p>\n",
    "\n",
    "<p> If you have any question or find any error in this document, DM/ping me at <span style=\"color:blue\"> @tmdhoctiengphap </span> or <span style=\"color:red\"> @ICT-Supporters </span> on the Discord of USTH Learning Support. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
