{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **k-Nearest Neighbors** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Tr·∫ßn Minh D∆∞∆°ng - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "Imagine you just moved to a new city and are looking for a good restaurant to try. You ask a few locals where they usually eat. If most of them recommend the same place, you‚Äôre likely to trust their judgment and go there. \n",
    "\n",
    "This is the core idea behind k-Nearest Neighbors (k-NN)‚Äîit makes predictions based on the **votes** of its closest \"neighbors.\"\n",
    "\n",
    "Watch this short video üëâüèª <a href = \"https://www.youtube.com/watch?v=0p0o5cmgLdE\">K Nearest Neighbors | Intuitive explained</a>\n",
    "\n",
    "## How k-NN Works:\n",
    "\n",
    "1. **Store all data points**: k-NN memorizes all training data without creating a fixed mathematical model (parametric models)\n",
    "\n",
    "2. **Find the k closest points**: Given a new input, k-NN calculates the distance between this point and all existing data points.\n",
    "\n",
    "3. **Vote for classification**: In classification, the majority class among the k nearest neighbors determines the new data point‚Äôs label.\n",
    "\n",
    "4. **Average for regression**: In regression, k-NN predicts the value by averaging the outputs of the k nearest neighbors.\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "### 1. **Initialization**\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/kNN1.png\" width = \"50%\">\n",
    "</center>\n",
    "\n",
    "Load the dataset, including their labels (classes)\n",
    "\n",
    "Enter data for the new input (query instance)\n",
    "\n",
    "### 2. **Calculate the distance**\n",
    "\n",
    "<center>\n",
    "<img src = \"../images/kNN2.png\" width = \"50%\">\n",
    "</center>\n",
    "\n",
    "Compute the distance between a query instance and all training examples using a chosen metric. Common metrics include:\n",
    "\n",
    "- **Euclidean Distance**:\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}$$\n",
    "- **Manhattan Distance**:\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i|$$\n",
    "- **Minkowski Distance** (generalized form):\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p}$$\n",
    "\n",
    "### 2. **Neighbor Selection**\n",
    "Identify the `k` training examples closest to the query instance.\n",
    "\n",
    "### 3. **Prediction**\n",
    "- **Classification**: Majority vote among neighbors.\n",
    "- **Regression**: Average of neighbors' target values.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing k\n",
    "- Smaller `k` (e.g., 1-3): Higher variance, sensitive to noise.\n",
    "- Larger `k`: Smoother decision boundaries but may underfit.\n",
    "- Use cross-validation to optimize `k`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "| **Advantages**                     | **Disadvantages**                          |\n",
    "|------------------------------------|--------------------------------------------|\n",
    "| No training phase                  | Computationally expensive for large data  |\n",
    "| Simple to implement                | Sensitive to irrelevant features          |\n",
    "| Naturally handles multi-class data | Requires feature scaling                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "**Task**: Predict the class label for a new data point using k-NN with `k=3`.\n",
    "\n",
    "**Dataset**:\n",
    "| Sample | Feature 1 | Feature 2 | Class |\n",
    "|--------|-----------|-----------|-------|\n",
    "| 1      | 2         | 4         | A     |\n",
    "| 2      | 3         | 5         | A     |\n",
    "| 3      | 5         | 1         | B     |\n",
    "| 4      | 7         | 3         | B     |\n",
    "| 5      | 6         | 2         | B     |\n",
    "\n",
    "**Query Point**: Feature 1 = 4, Feature 2 = 3\n",
    "\n",
    "**Steps**:\n",
    "1. Calculate distances between the query and all samples.\n",
    "2. Select the 3 nearest neighbors.\n",
    "3. Assign the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Step 1: Compute Distances\n",
    "Using **Euclidean Distance**:\n",
    "\n",
    "- Distance to Sample 1: \n",
    "  $$d = \\sqrt{(4-2)^2 + (3-4)^2} = \\sqrt{4 + 1} = 2.24$$\n",
    "- Distance to Sample 2: \n",
    "  $$d = \\sqrt{(4-3)^2 + (3-5)^2} = \\sqrt{1 + 4} = 2.24$$\n",
    "- Distance to Sample 3: \n",
    "  $$d = \\sqrt{(4-5)^2 + (3-1)^2} = \\sqrt{1 + 4} = 2.24$$\n",
    "- Distance to Sample 4: \n",
    "  $$d = \\sqrt{(4-7)^2 + (3-3)^2} = \\sqrt{9 + 0} = 3.0$$\n",
    "- Distance to Sample 5: \n",
    "  $$d = \\sqrt{(4-6)^2 + (3-2)^2} = \\sqrt{4 + 1} = 2.24$$\n",
    "\n",
    "### Step 2: Identify Nearest Neighbors\n",
    "Nearest 3 samples: **1, 2, 3, 5** (tie at distance 2.24).\n",
    "\n",
    "### Step 3: Majority Vote\n",
    "- Classes: [A, A, B, B] ‚Üí Majority class = **B**\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Training data\n",
    "X_train = np.array([[2,4], [3,5], [5,1], [7,3], [6,2]])\n",
    "y_train = np.array(['A', 'A', 'B', 'B', 'B'])\n",
    "\n",
    "# Query point\n",
    "query = np.array([[4,3]])\n",
    "\n",
    "# Initialize k-NN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "prediction = knn.predict(query)\n",
    "print(f\"Predicted class: {prediction[0]}\")\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Predicted class: B\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Manual Implementation (Optional)\n",
    "\n",
    "```python\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "# Compute distances\n",
    "distances = []\n",
    "for i, sample in enumerate(X_train):\n",
    "    d = euclidean_distance(sample, query[0])\n",
    "    distances.append((d, y_train[i]))\n",
    "\n",
    "# Sort and select top-k\n",
    "distances.sort(key=lambda x: x[0])\n",
    "neighbors = [item[1] for item in distances[:3]]\n",
    "\n",
    "# Majority vote\n",
    "from collections import Counter\n",
    "votes = Counter(neighbors)\n",
    "print(f\"Manual prediction: {votes.most_common(1)[0][0]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Tips\n",
    "- **Normalize features** to prevent dominance of high-magnitude features.\n",
    "- Use **KD-Trees** or **Ball Trees** for efficient neighbor search in large datasets.\n",
    "- Experiment with `k` and distance metrics using cross-validation.\n",
    "\n",
    "This document was created in Jupyter Notebook by Tr·∫ßn Minh D∆∞∆°ng (tmd). For questions or corrections, contact @tmdhoctiengphap on Discord.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
