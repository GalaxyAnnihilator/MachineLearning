{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center style=\"color:red\"> **k-Nearest Neighbors** </center>  \n",
    "\n",
    "<h4 style=\"text-align:right\">By Trần Minh Dương - Learning Support</h4>  \n",
    "\n",
    "# Overview  \n",
    "\n",
    "k-Nearest Neighbors (k-NN) is a simple, instance-based supervised learning algorithm used for classification and regression. It operates on the principle that similar data points exist in close proximity. Unlike parametric models, k-NN makes no assumptions about the underlying data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "### 1. **Distance Calculation**\n",
    "Compute the distance between a query instance and all training examples using a chosen metric. Common metrics include:\n",
    "\n",
    "- **Euclidean Distance**:\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}$$\n",
    "- **Manhattan Distance**:\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i|$$\n",
    "- **Minkowski Distance** (generalized form):\n",
    "  $$d(\\mathbf{x}, \\mathbf{y}) = \\left( \\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p}$$\n",
    "\n",
    "### 2. **Neighbor Selection**\n",
    "Identify the `k` training examples closest to the query instance.\n",
    "\n",
    "### 3. **Prediction**\n",
    "- **Classification**: Majority vote among neighbors.\n",
    "- **Regression**: Average of neighbors' target values.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing k\n",
    "- Smaller `k` (e.g., 1-3): Higher variance, sensitive to noise.\n",
    "- Larger `k`: Smoother decision boundaries but may underfit.\n",
    "- Use cross-validation to optimize `k`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pros and Cons\n",
    "\n",
    "| **Advantages**                     | **Disadvantages**                          |\n",
    "|------------------------------------|--------------------------------------------|\n",
    "| No training phase                  | Computationally expensive for large data  |\n",
    "| Simple to implement                | Sensitive to irrelevant features          |\n",
    "| Naturally handles multi-class data | Requires feature scaling                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "**Task**: Predict the class label for a new data point using k-NN with `k=3`.\n",
    "\n",
    "**Dataset**:\n",
    "| Sample | Feature 1 | Feature 2 | Class |\n",
    "|--------|-----------|-----------|-------|\n",
    "| 1      | 2         | 4         | A     |\n",
    "| 2      | 3         | 5         | A     |\n",
    "| 3      | 5         | 1         | B     |\n",
    "| 4      | 7         | 3         | B     |\n",
    "| 5      | 6         | 2         | B     |\n",
    "\n",
    "**Query Point**: Feature 1 = 4, Feature 2 = 3\n",
    "\n",
    "**Steps**:\n",
    "1. Calculate distances between the query and all samples.\n",
    "2. Select the 3 nearest neighbors.\n",
    "3. Assign the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution\n",
    "\n",
    "### Step 1: Compute Distances\n",
    "Using **Euclidean Distance**:\n",
    "\n",
    "- Distance to Sample 1: \n",
    "  $$d = \\sqrt{(4-2)^2 + (3-4)^2} = \\sqrt{4 + 1} = 2.24$$\n",
    "- Distance to Sample 2: \n",
    "  $$d = \\sqrt{(4-3)^2 + (3-5)^2} = \\sqrt{1 + 4} = 2.24$$\n",
    "- Distance to Sample 3: \n",
    "  $$d = \\sqrt{(4-5)^2 + (3-1)^2} = \\sqrt{1 + 4} = 2.24$$\n",
    "- Distance to Sample 4: \n",
    "  $$d = \\sqrt{(4-7)^2 + (3-3)^2} = \\sqrt{9 + 0} = 3.0$$\n",
    "- Distance to Sample 5: \n",
    "  $$d = \\sqrt{(4-6)^2 + (3-2)^2} = \\sqrt{4 + 1} = 2.24$$\n",
    "\n",
    "### Step 2: Identify Nearest Neighbors\n",
    "Nearest 3 samples: **1, 2, 3, 5** (tie at distance 2.24).\n",
    "\n",
    "### Step 3: Majority Vote\n",
    "- Classes: [A, A, B, B] → Majority class = **B**\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Training data\n",
    "X_train = np.array([[2,4], [3,5], [5,1], [7,3], [6,2]])\n",
    "y_train = np.array(['A', 'A', 'B', 'B', 'B'])\n",
    "\n",
    "# Query point\n",
    "query = np.array([[4,3]])\n",
    "\n",
    "# Initialize k-NN model\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "prediction = knn.predict(query)\n",
    "print(f\"Predicted class: {prediction[0]}\")\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Predicted class: B\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Manual Implementation (Optional)\n",
    "\n",
    "```python\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "# Compute distances\n",
    "distances = []\n",
    "for i, sample in enumerate(X_train):\n",
    "    d = euclidean_distance(sample, query[0])\n",
    "    distances.append((d, y_train[i]))\n",
    "\n",
    "# Sort and select top-k\n",
    "distances.sort(key=lambda x: x[0])\n",
    "neighbors = [item[1] for item in distances[:3]]\n",
    "\n",
    "# Majority vote\n",
    "from collections import Counter\n",
    "votes = Counter(neighbors)\n",
    "print(f\"Manual prediction: {votes.most_common(1)[0][0]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Tips\n",
    "- **Normalize features** to prevent dominance of high-magnitude features.\n",
    "- Use **KD-Trees** or **Ball Trees** for efficient neighbor search in large datasets.\n",
    "- Experiment with `k` and distance metrics using cross-validation.\n",
    "\n",
    "This document was created in Jupyter Notebook by Trần Minh Dương (tmd). For questions or corrections, contact @tmdhoctiengphap on Discord.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
